---
title: "Assets Portfolio Analysis"
author: "Felipe Tufaile, Vinicius de Camargo, Helena Funari, Rodrigo Zamengo"
date: "11/12/2022"
output: html_document
---


## Summary

This document studies the time series of a group of 6 financial instruments with the aim of suggesting an investment portfolio composed of the assets in question in calculated proportion. The proportion of each asset, in turn, will be calculated by analyzing the assets log-return series, as well as modeling its volatility and calculating the Sharpe index. Finally, the study will end calculating the return of the portfolio, as well as its volatility, and compare it to a benchmark, which in this case will be considered to be, primary, the **IBOVESPA** index, but also the **CDI** (Interbank Deposit Certificate) index. 
For the purpose fo this work, it will analyzed the following assets:


  **Company Stocks**\
  
  - Magazine Luiza S.A. (MGLU3.SA);\
  
  - Vale S.A. (VALE3.SA);\
  
  - Petróleo Brasileiro S.A. - Petrobras (PETR4.SA);\ 
  
  - B3 S.A. - Brasil, Bolsa, Balcão (B3SA3.SA);\
  
  - Eneva S.A. (ENEV3.SA);\
  
  
  **Real Estate Investment Fund**\
  
  - CSHG Real Estate - Fundo de Investimento Imobiliario - FII (HGRE11.SA);


```{r configs, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, include = TRUE, message = FALSE)
```

## Loading libraries

```{r libraries}

## Set working directory
setwd("~/Insper/Stock_Portfolio_Analysis")

library(dplyr)
library(tidyverse)
library(ggthemes)
library(fpp3)
library(forecast)
library(gridExtra)
library(ggpubr)
library(tseries)
library(prophet)
library(rugarch)
library(yfR)
library(timetk)
library(tidyquant)
```


## Functions definition

```{r model_builder}

##-- Defining function: Create a GARCH MODEL --##

## Creating a function that adjust a Garch model for a given set of parameters / data
GModels <- function(parms, series, prog = NULL) {
  
  if (!is.null(prog)) prog()
  
  if(parms$model=="iGARCH"){
    
    ## Configuring iGARCH model
    garch_model = ugarchspec(
    variance.model = list(model=parms$model, garchOrder=c(parms$m, parms$n)),
    mean.model = list(armaOrder = c(parms$p, parms$q), include.mean = TRUE),
    distribution.model = parms$dist)
  }
  
  else {
    
    ## Configuring other models
    garch_model = ugarchspec(
    variance.model = list(model=parms$model, submodel=parms$submodel, garchOrder=c(parms$m, parms$n)),
    mean.model = list(armaOrder = c(parms$p, parms$q), include.mean = TRUE),
    distribution.model = parms$dist
    )
  }
  
  
  ## Suppressing warning when the model doesnt converge
  suppressWarnings({fit <- ugarchfit(spec=garch_model, data=series, solver='solnp', solver.control=list(tol = 5e-8))})
  
  ## Return the fit model
  fit
}



##-- Defining function: Find Best GARCH --##

## Creating a function that finds the best Garch model for each asset
find_best_garch <- function(asset, grid, df) {
     
  ## Selects the asset and it squared log return
  ret2 <- df_log_returns %>% filter(ticker==asset) %>% select(ret_closing_prices) %>% pull()
  
  ## Print which asset is being adjusted
  usethis::ui_info("Adjusting models for {asset}...")
 
  ## Show progress
  progressr::with_progress({
    prog <- progressr::progressor(nrow(grid))
    models <- grid %>%
      group_split(id) %>%
      purrr::map(GModels, series=ret2, prog)
    })
  
  safe_info <- purrr::possibly(infocriteria, tibble::tibble())
  
  ## Get model information
  suppressWarnings({
   info <- purrr::map(models, safe_info) %>%
     purrr::map(tibble::as_tibble, rownames = "criteria") %>%
     dplyr::bind_rows(.id = "id")
   })
  
  return(info)
  
  ## Collecting models info
  best <- info %>%
    dplyr::inner_join(grid, "id") %>%
    tidyr::pivot_wider(names_from = criteria, values_from = V1) %>%
    janitor::clean_names() %>%
    dplyr::arrange(akaike)
  
  ## Selecting the best parameters
  usethis::ui_info(c(
    "Best model:",
    "p <- {best$p[1]}",
    "q <- {best$q[1]}",
    "m <- {best$m[1]}",
    "n <- {best$n[1]}"
    ))
  
  best
}



##-- Defining function: Volatility Backtest --##

volatility_backtest <- function(parms) {
  
  ## Selects the asset and it squared log return
  ret <- df_log_returns %>% filter(ticker==parms$ticker) %>% pull(ret_closing_prices)
  
  if(parms$model=="iGARCH"){
    
    ## Configuring iGARCH model
    garch_model = ugarchspec(
    variance.model = list(model=parms$model, garchOrder=c(parms$m, parms$n)),
    mean.model = list(armaOrder = c(parms$p, parms$q), include.mean = T),
    distribution.model = parms$dist)
  }
  
  else {
    
    ## Configuring other models
    garch_model = ugarchspec(
    variance.model = list(model=parms$model, submodel=parms$submodel, garchOrder=c(parms$m, parms$n)),
    mean.model = list(armaOrder = c(parms$p, parms$q), include.mean = T),
    distribution.model = parms$dist
    )
  }
  
  
  ## Performing backtest
  suppressWarnings({garchroll <- ugarchroll(garch_model, data=ret, n.start = 200,
                                            refit.window = c("recursive", "moving"),
                                            refit.every = 50)})
  
  
  ## Collecting the distribution coefficient
  predictions <- as.data.frame(garchroll)
  
  # Prediction error for the mean
  error  <- predictions$Realized - predictions$Mu 
  
  # Prediction error for the variance
  volatility_error  <- error^2 - predictions$Sigma^2 
  
  # Mean of prediction error
  return(mean(volatility_error^2))
  
}




##-- Defining function: Volatility Forecast --##

volatility_forecast <- function(parms, n_steps = 5) {
  
  ## Selects the asset and it squared log return
  ret <- df_log_returns %>% filter(ticker==parms$ticker) %>% pull(ret_closing_prices)
  
  if(parms$model=="iGARCH"){
    
    ## Configuring iGARCH model
    garch_model = ugarchspec(
    variance.model = list(model=parms$model, garchOrder=c(parms$m, parms$n)),
    mean.model = list(armaOrder = c(parms$p, parms$q), include.mean = T),
    distribution.model = parms$dist)
  }
  
  else {
    
    ## Configuring other models
    garch_model = ugarchspec(
    variance.model = list(model=parms$model, submodel=parms$submodel, garchOrder=c(parms$m, parms$n)),
    mean.model = list(armaOrder = c(parms$p, parms$q), include.mean = T),
    distribution.model = parms$dist
    )
  }
  
  
  ## Suppressing warning when the model doesn't converge
  suppressWarnings({fit <- ugarchfit(spec=garch_model, 
                                     data=ret, 
                                     solver='solnp', 
                                     solver.control=list(tol=5e-8))})
  
  
  ## Collecting the distribution coefficient
  if (parms$dist == "std") {
    shape <- as.numeric(fit@fit$coef["shape"])
  } 
  
  else {
    shape <- NA_real_
  }

  
  ## Forecasting the next n_steps
  suppressWarnings({forecasts <- ugarchforecast(fit, n.ahead=n_steps)@forecast})
  
  ## Returning results
  return(list(
    ticker=parms$ticker, 
    serie=as.numeric(forecasts$seriesFor), 
    volatility=as.numeric(forecasts$sigmaFor),
    shape=shape,
    mat_coef=fit@fit$matcoef
    )
  )
  
}



##-- Defining function: Portfolio Simulation --##

portfolio_simulation <- function(nruns, assets, return_xts) {
  
  ## Creating an empty dataset
  portfolios <- tibble(
                   id = as.numeric(),
                   B3SA3.SA = as.numeric(),
                   ENEV3.SA = as.numeric(),
                   HGRE11.SA = as.numeric(),
                   MGLU3.SA = as.numeric(),
                   PETR4.SA = as.numeric(),
                   VALE3.SA = as.numeric(),
                   portfolio_return = as.numeric(),
                   portfolio_volatility = as.numeric(),
                   portfolio_sharpe_ratio = as.numeric()
                  )
  
  ## Creating a progress bar
  # pb <- txtProgressBar(min = 1, max = nruns, style = 3)
  
  ## Calculating mean return
  mean_ret <- colMeans(return_xts, na.rm = TRUE)
  
  ## Calculating the covariance matrix
  cov_mat <- cov(return_xts) * 252
  
  ## Iterating nruns times
  for (i in 1:nruns){

    ## Defining random weights and normalizing it so the sum adds up to 1
    wts <- runif(length(assets))
    wts <- wts/sum(wts)
  
    ## Calculating portfolio returns  
    port_ret <- sum(wts * mean_ret)
    
    ## Multiplying by the number of working days
    port_ret <- ((port_ret + 1)^252) - 1
    
    ## Calculate the portfolio risk (volatility / standard deviation)
    port_sd <- as.numeric(sqrt(t(wts) %*% (cov_mat %*% wts)))
    
    ## Calculating Sharpe Ratio
    sharpe_ratio <- port_ret / port_sd
    
    portfolios <- portfolios %>%
      bind_rows(.,
                tibble(
                  id = i,
                  B3SA3.SA = wts[names(mean_ret)=="B3SA3.SA"],
                  ENEV3.SA = wts[names(mean_ret)=="ENEV3.SA"],
                  HGRE11.SA = wts[names(mean_ret)=="HGRE11.SA"],
                  MGLU3.SA = wts[names(mean_ret)=="MGLU3.SA"],
                  PETR4.SA = wts[names(mean_ret)=="PETR4.SA"],
                  VALE3.SA = wts[names(mean_ret)=="VALE3.SA"],
                  portfolio_return = port_ret,
                  portfolio_volatility = port_sd,
                  portfolio_sharpe_ratio = sharpe_ratio
                )
               )
    
    # Print progress
	  # setTxtProgressBar(pb, i)
  }
  
  return(portfolios)
}


##-- Defining function: Calculate Beta --##

calculate_beta <- function(asset) {
  
  ## Filtering the corresponding return series
  series <- df_log_returns %>%
    dplyr::filter(ticker==asset) %>%
    dplyr::mutate(ibov_return = df_bvsp$ibov)
  
  ## Calculating beta
  beta <- as.numeric(lm(ret_closing_prices ~ ibov_return, data = series)$coefficients[2])

  return (beta)
}
```



## Plotting stock price series

The following plot shows the closing price for each of the assets mentioned in the summary of this document. The time series initial date was set to 2019-11-01 (Friday) and the final date the 2022-10-31 (Monday), configuring a time period of three years with price information in daily basis. Furthermore, by starting at the end of the year 2019, the prices fluctuation will capture the stock market crash due to the world health crisis announcement (COVID pandemics) on march 2020 and also the impact of the pandemic on the economy in subsequent months.

During the pandemics, the fact that people were secluded at home caused the demand for delivery to increase significantly, mostly because delivery would be the most convenient option during lock-down but also because buying material goods would be a way of getting rid of boredom of being locked-down. In that context, eCommerce like **MGLU3.SA**, saw their sales volume boom resulting an increase of their stock prices in subsequently months. However, this movement also resulted an inflationary pressure in the price of goods which, along with the fact that many people were laid off during the pandemics, caused a decrease in the demand of delivery as the number of COVID cases saw a deceleration in the end of 2021. Consequently causing the stock prices of eCommerce companies to lose value.

For **VALE3.SA**, the initial high value of the stock is a reflection of the attempt to restart the economy with the slowdown of the pandemic. This moment lasted until the third quarter of 2021, when a drop in the price of iron ore on the global market caused a decrease in value of the stock. On the other hand, the value of the asset rose again at the beginning of 2022 due to the lack of competition on the global market since the war in Ukraine started, along with the expectation of an upturn in the Chinese economy. However, it soon fell again when the Chinese government reestablished restrictions to contain further advances of COVID.

On the other hand, **PETR4.SA** saw its stock value increase since the end of 2020 due to constant readjustments of local oil prices to international prices, motivated by the rise of the dollar, and to the increase in oil prices, in a more general way, due to the war between Russia and Ukraine since the beginning of 2022. More recently, the loss in value of the stock price was driven by risk aversion towards the future of the company with the election of Lula (even before the actual result). The financial market sees the election of Lula as a negative sign for the company since it would result in more interference, by the state, in company's business.

**B3SA3.SA** was also impacted by the effects of the pandemic. The unprecedented drop in the value of company shares in general, added to the low return on fixed-income investments, opened an opportunity for many people to start investing in variable income with the aim of obtaining significant financial gains. With more people investing in variable income, B3 saw its revenue increase. However, with the gradual realization of profit by investors, added to the rise in the basic interest rate in order to contain the inflationary pressures caused by the pandemic, many investors migrated from variable income to fixed income, gradually reducing the company's ability to generate profit. In the beginning of 2022, in turn, reports showed that the company was favored by a large inflow of foreign money, however, in the beginning of April 2022, the company released an inform announcing a correction in the measurement of foreign money inflow which, in fact, indicated a decrease in external investments. This news caused a decrease on the asset value at the time, which has been recently recovered with positive performance.

For **ENEV3.SA** the value of the stock decreased with the increase of the interest rate in Brazil since it slowed down several projects that the company had in its horizon reaching the lowest value in the beginning of 2022. Since then, the sequence of positive performance along with the promise of being able to end the payment of its accumulated debt in 2023 (damage caused by the scandal involving Eike Batista) has led the stock price to increase. However, the recent resignation of the company's CEO, Pedro Zinner, who has been instrumental in the company's financial restructuring, caused the asset price to fall again.

Finally, **HGRE11.SA**, a real estate fund whose investments are mainly earmarked for properties in the corporate slab segment, saw the vacancy level of the fund's properties increase significantly during the pandemic, causing a negative impact on the value of the asset. However, the resumption of face-to-face work by companies has caused the vacancy value of properties to decrease recently. Mainly for properties located on important avenues such as Faria Lima.


```{r reading_stocks, out.width="100%"}

## Defining the start date of the time series
start_date <- '2019-11-01'

## Defining the end date of the time series
end_date <- '2022-10-31'

## Defining the stocks that will be considered for this analysis
tickers <- c("HGRE11.SA", "MGLU3.SA", "VALE3.SA", "PETR4.SA", "B3SA3.SA", "ENEV3.SA")

## Reading the log-return of each chosen stock
#df_log_returns <- yfR::yf_get(
#  tickers=tickers,
#  first_date = start_date,
#  type_return = "log",
#  freq_data = "daily", 
#  do_complete_data = TRUE
#)


## Reading the log-return of the assets closing price from a csv file
df_log_returns <- read_csv("20221116_stocks.csv") %>%
    mutate(ret_closing_prices = as.numeric(ret_closing_prices)) %>%
    filter(ref_date >= start_date & ref_date <= end_date)

## Converting the dataframe into a time series object using tsibble
df_log_returns_tsibble <- df_log_returns %>% 
  as_tsibble(key = ticker, index = ref_date, regular = FALSE)

## Plotting the closing price time series
df_log_returns_tsibble %>%
  autoplot(price_close, colour = "black") +
  facet_wrap(~ticker, scales = "free_y", ncol = 1) +
  ggtitle("Closing price for selected assets", 
          "Data ranging from Nov-2019 to Nov-22") +
  labs(x="Reference Date", y = "Closing Price (R$)")
```


## Plotting the log return series of the stock price

Looking at the graph of the log return on the closing price of the assets, it is noted that the visual aspect of the return series of "MGLU3.SA" is the one that suggests the highest volatility, especially in the last 12 months. On the other hand, the visual aspect of the real estate investment fund log return series suggests that the asset has the lowest volatility among those studied. That statement will be verified later on this document, however it gives an educated guess about what to expect when calculating the volatility of the stock prices. 

```{r plotting_close_price_return, out.width="100%"}
## Plotting the closing price log return time series
df_log_returns_tsibble %>%
  autoplot(ret_closing_prices, colour = "black") +
  facet_wrap(~ticker, scales = "fixed", ncol = 1) +
  ggtitle("Log return of closing prices of selected assets", 
          "Data ranging from Nov-2019 to Nov-22") +
  labs(x="Reference Date", y = "Log Return of Closing Price")
```

```{r filtering_domain}
## Calculating the first not null day: when calculating log returns the first day becomes null
first_date <- df_log_returns_tsibble %>%
  dplyr::group_by(ticker) %>% 
  dplyr::filter(ref_date == min(ref_date)) %>% 
  dplyr::ungroup() %>% 
  with(max(ref_date))

## Filtering out all data before "first_date"
df_log_returns_cln_tsibble <- df_log_returns_tsibble %>%
    dplyr::filter(ref_date > first_date)
```


## Plotting the ACF and PACF functions of the log-returns


The plot of the **ACF** and **PACF** functions suggests the presence of some autocorrelation for all log-return series, especially for "ENEV3.SA", "HGRE11.SA" and "PETR4.SA". That means that the log-return of the chosen stocks would not be considered **stationary** and, therefore, they would have unit roots. Consequently, it would be necessary to fit a **ARMA** model to make the log return series stationary. However, the **KPSS** and an **ADF** test suggests no presence of unit roots in any of the log return series, as it can be seen in the sequence. That confirms one of the usual facts in finance that is that the return series are usually not auto-correlated and are stationary. Given the result of the **KPSS** and **ADF** tests, the log return series will be considered stationary.

```{r plotting_acf, out.width="100%"}
df_log_returns_cln_tsibble %>%
  ACF(ret_closing_prices) %>%
  autoplot() +
  ggtitle("ACF plot of the log return of closing prices of selected assets") +
  labs(x="Lag", y = "ACF")
```



```{r plotting_pacf, out.width="100%"}
df_log_returns_cln_tsibble %>%
  PACF(ret_closing_prices) %>%
  autoplot() +
  ggtitle("PACF plot of the log return of closing prices of selected assets") +
  labs(x="Lag", y = "PACF")
```

## KPSS test on each stock series


The Kwiatkowski-Phillips-Schmidt-Shin (**KPSS**) test, (Kwiatkowski et al., 1992), is suggested by Robert Hyndman in his book (Hynman, 2021) as an alternative to verift the existance of unit roots in a time series. The test assumes that the data is stationary as the null hypothesis. Therefore, small p-values suggest that the data has an unit root and that differencing is required. Considering the log_return series, the **KPSS** test yields a p-value higher than 0.05 for each stock series, therefore, failing to reject null hypothesis. With that, all log-return series can be considered stationary as the graphical analysis of the ACF and PACF functions suggested.

```{r kpss_test_transformed}
## Performing KPSS test
fabletools::features(.tbl=df_log_returns_cln_tsibble %>% select(ticker, ret_closing_prices) %>% group_by(ticker), 
                     .var=ret_closing_prices, 
                     features=list(unitroot_kpss, unitroot_nsdiffs))

```

## Augmented Dickey-Fuller (ADF) test on each stock series

The null hypothesis of the **ADF** assumes that there is an unit root in the time series (therefore the series is non-stationary), whereas the alternate hypothesis assumes that the series are stationary. Since the p-values obtained for both tests are low (less than 0.05) the null hypothesis is rejected indicating that the series are stationary.


```{r adf_test_original}
## Remove the first null datat point from the log return series
df_log_returns <- df_log_returns %>%
  dplyr::filter(ref_date > first_date)

## Pulling log return data for each log return series
B3SA3 <- df_log_returns %>% filter(ticker == 'B3SA3.SA') %>% select(ret_closing_prices) %>% pull()
ENEV3 <- df_log_returns %>% filter(ticker == 'ENEV3.SA') %>% select(ret_closing_prices) %>% pull()
HGRE11 <- df_log_returns %>% filter(ticker == 'HGRE11.SA') %>% select(ret_closing_prices) %>% pull()
MGLU3 <- df_log_returns %>% filter(ticker == 'MGLU3.SA') %>% select(ret_closing_prices) %>% pull()
PETR4 <- df_log_returns %>% filter(ticker == 'PETR4.SA') %>% select(ret_closing_prices) %>% pull()
VALE3 <- df_log_returns %>% filter(ticker == 'VALE3.SA') %>% select(ret_closing_prices) %>% pull()

## Printing test results
print(tseries::adf.test(B3SA3, alternative = c("stationary", "explosive"), k = trunc((length(B3SA3)-1)^(1/3))))
print(tseries::adf.test(ENEV3, alternative = c("stationary", "explosive"), k = trunc((length(ENEV3)-1)^(1/3))))
print(tseries::adf.test(HGRE11, alternative = c("stationary", "explosive"), k = trunc((length(HGRE11)-1)^(1/3))))
print(tseries::adf.test(MGLU3, alternative = c("stationary", "explosive"), k = trunc((length(MGLU3)-1)^(1/3))))
print(tseries::adf.test(PETR4, alternative = c("stationary", "explosive"), k = trunc((length(PETR4)-1)^(1/3))))
print(tseries::adf.test(VALE3, alternative = c("stationary", "explosive"), k = trunc((length(VALE3)-1)^(1/3))))
```


## Squared log return series

The calculation of the squared log return series of the assets prices show some interesting facts. 
Firstly, by squaring the log returns, the points in time that faced more volatility becomes more evident. For instance, it becomes even more clear the moment when the financial market crashed in March 2020 due to the announcement of the COVID pandemics. Another moment of high volatility that became more evident with the squared log return series is the peak that happen in "PETR4.SA" series in the first quarter of 2021. A quick research about this phenomena shows that "PETR4.SA" stock prices dropped nearly 21%, after president Jair Bolsonaro appointed General Joaquim Silva e Luna to replace the company's current president, Roberto Castello Branco. For Magazine Luiza, on other hand, a high volatility peak in the mid of the last quarter of 2021 was the result of the company's earnings release relative to the third quarter of the same year which indicated a decrease of nearly 90% of the gross revenue compared to the same period in the year before.
Secondly, the squared log return also highlight another important usual fact that is common to financial data which is the presence of **conditional heteroscedasticity** or **volatility gathering** - periods when low and high fluctuations alternates. This last fact can be more easily diagnosed plotting the **ACF** and **PACF** functions of the squared log returns which denotes the existence of autocorrelation.


```{r calculating_return_squared, out.width="100%"}
df_log_returns_cln_tsibble <- df_log_returns_cln_tsibble %>%
  dplyr::mutate(ret2 = ret_closing_prices^2) 

df_log_returns_cln_tsibble %>%
  autoplot(ret2, colour = "black") +
  facet_wrap(~ticker, ncol = 1, scales = "fixed") +
  ggtitle("Squared log return of closing prices of selected assets", 
          "Data ranging from Nov-2019 to Nov-22") +
  labs(x="Reference Date", y = "Squared log return")
```


```{r plotting_acf_log_return_sqrd, out.width="100%"}
df_log_returns_cln_tsibble %>%
  ACF(ret2) %>% 
  autoplot +
  ggtitle("ACF plot of squared log return of closing prices of selected assets") +
  labs(x="Lag", y = "ACF")
```

```{r plotting_pacf_log_return_sqrd, out.width="100%"}
df_log_returns_cln_tsibble %>% 
  PACF(ret2) %>% 
  autoplot() +
  ggtitle("PACF plot of squared log return of closing prices of selected assets") +
  labs(x="Lag", y = "PACF")
```


The existence of conditional heteroscedasticity could also be verified applying a Ljun-Box test on the squared log return series. If the Ljung-Box test returns a large p-value the null hypothesis is not rejected, also suggesting that the residuals are **white noise**. Ljung-Box hypothesis:\

- H0: The residuals **behave like white noise**;
- H1: The residuals **do not behave like white noise**;

Looking at the p-values obtained with test (for each individual series) it noticeable that they are all less than 0.05 for all series meaning that they have auto-correlation as expected.

```{r ljung_box_r2, out.width="100%"}
## Pulling log return data for each log return series
B3SA3sqr <- df_log_returns %>% filter(ticker == 'B3SA3.SA') %>% mutate(ret2 = ret_closing_prices^2) %>% select(ret2) %>% pull()
ENEV3sqr <- df_log_returns %>% filter(ticker == 'ENEV3.SA') %>% mutate(ret2 = ret_closing_prices^2) %>% select(ret2) %>% pull()
HGRE11sqr <- df_log_returns %>% filter(ticker == 'HGRE11.SA') %>% mutate(ret2 = ret_closing_prices^2) %>% select(ret2) %>% pull()
MGLU3sqr <- df_log_returns %>% filter(ticker == 'MGLU3.SA') %>% mutate(ret2 = ret_closing_prices^2) %>% select(ret2) %>% pull()
PETR4sqr <- df_log_returns %>% filter(ticker == 'PETR4.SA') %>% mutate(ret2 = ret_closing_prices^2) %>% select(ret2) %>% pull()
VALE3sqr <- df_log_returns %>% filter(ticker == 'VALE3.SA') %>% mutate(ret2 = ret_closing_prices^2) %>% select(ret2) %>% pull()

## Printing test results
print(x=Box.test(B3SA3sqr, lag=12, fitdf=1, type="Ljung-Box"))
print(x=Box.test(ENEV3sqr, lag=12, fitdf=1, type="Ljung-Box"))
print(x=Box.test(HGRE11sqr, lag=12, fitdf=1, type="Ljung-Box"))
print(x=Box.test(MGLU3sqr, lag=12, fitdf=1, type="Ljung-Box"))
print(x=Box.test(PETR4sqr, lag=12, fitdf=1, type="Ljung-Box"))
print(x=Box.test(VALE3sqr, lag=12, fitdf=1, type="Ljung-Box"))
```


## Volatility models

After confirming that the return series are not autocorrelated and that the squared returns, in turn, have autocorrelation, the volatility of each return series will be modeled using GARCH models. In the code below, different models will be tested considering combination of **model** type (between fGARCH and iGARCH), **submodel** (TGARCH, GARCH), values of **m** and **n** (between 0 or 1) and types of distribution for the returns (norm, snorm, std and sstd). The best models will then be selected considering the ones with the lowest **Akaike Criteria**.

```{r run_garch_models}

## Creating a grid search
mtr <- crossing(m=c(0, 1), n=c(0, 1), p=c(0), q=c(0), dist=c("snorm", "norm", "std", "sstd"))
GridSearch <- bind_rows(cbind(tibble(model="fGARCH", submodel="TGARCH"), mtr),
                        cbind(tibble(model="fGARCH", submodel="GARCH"), mtr),
                        cbind(tibble(model="iGARCH", submodel=NULL), mtr %>% filter(n==1))) %>%
  tibble::rownames_to_column("id")

## Running-hyper parameter tuning
best_models <- tickers %>% 
  purrr::set_names() %>% 
  purrr::map(find_best_garch, grid=GridSearch) %>% 
  dplyr::bind_rows(.id = "ticker")

## Plotting the best model per asset
final_models <- best_models %>%
    filter(criteria == 'Akaike') %>%
    group_by(ticker) %>%
    slice(which.min(V1)) %>%
    merge(., GridSearch, by.x="id", by.y="id", all.x=F, all.y=F) %>%
    select(ticker, model, submodel, m, n, p, q, dist, criteria, V1) %>%
    rename(criteria_value=V1)

final_models
```


## Models backtest

The rugarch package also allows backtesting by either applying a sliding window (fixed length) or an expanding window (variable length). The test is useful for comparing the performance of two (or more) models against each other in order to decide which model is the best. For time sake, the best model for each asset was chosen by comparing the **Akaike Criteria** (see calculation above), but it could also be chosen through backtesting. Nevertheless, the following code will perform a "backtest" on the best model for each asset in order to calculate its mean volatility error.  

```{r}
## Creating table df_backtest
df_backtest <- tibble(ticker=as.character(), mean_error=as.numeric())

for (asset in tickers){
  
  ## Updating df_backtest table
  df_backtest <- df_backtest %>%
    bind_rows(.,tibble(ticker=asset, mean_error=volatility_backtest(parms=final_models[final_models$ticker==asset,])))
}

df_backtest
```

## Comparing volatilities

The following calculations show the "Value at Risk" (the maximum amount expected to be lost over a given time horizon) at 95% of confidence level. The formula used for the calculation is defined as:


\begin{align*}
VaR_{95} = 1.65 * h_{t}(1) * M
\end{align*}


Where $h_{t}(1)$ is the estimated volatility one step ahead and $M$ is the total amount invested, which, in this case, will be R$ 1 million for each asset. As the only variable in the equation above is the volatility term, the greatest the **VaR** value the greates the volatility. Therefore, it possible to compare the volatility of the different assets studies in this document by comparing the respective **VaR** one step ahead.

Looking at the VaRs, it is possible to notice that **HGRE11.SA** has the lowest values, whereas **PETR4.SA** has the highest value (little over four times higher than the lowest value). The reason for this outcome is probably due to two main reasons:

 - Firstly, the fact that **HGRE11.SA** is a real estate fund gives it a lower volatility since the value of the asset is more easily estimated. That is, knowing the vacancy rate and the rent value of each property that is in the fund portfolio gives it a good estimate of the asset value. Also, as the fund usually diversifies its investment in more than one property, vacancy increase in one property would be amortized by the payment of rents of the other properties in the portfolio. Therefore, the speculation around the asset true value would vary much less if compared to the value of a company;\
 
 - Secondly, looking at the squared log return plot one may notice that **PETR4.SA** has some periods in time when the volatility of its return is greater when compared to other stocks. Therefore, even though the price return of stocks like **MGLU3.SA** seems to be constantly varying significantly, the magnitude of the volatility of **PETR4.SA** is greater during periods of high volatility (e. g. during the stock market crash and after the change of the CEO). 

```{r value_at_risk}
## Creating table to hold results
tb_forecast_var <- tibble(ticker=as.character(),
                          mu=as.numeric(),
                          omega=as.numeric(),
                          beta1=as.numeric(),
                          shape=as.numeric(),
                          var95=as.numeric())

for (asset in tickers){
  
  ## Predicinting the volatility one step ahead
  predictions <- volatility_forecast(parms=final_models[final_models$ticker==asset,], n_steps=1)
  
  ## Calculating VaR95% considering an investment of R$ 1 million 
  var <- round(predictions$volatility*1.65*1000000, 2)
  
  ## Updating table of results
  tb_forecast_var <- tb_forecast_var %>%
    bind_rows(., tibble(ticker=asset,
                        mu=as_tibble(predictions$mat_coef)[1,1]$` Estimate`,
                        omega=as_tibble(predictions$mat_coef)[2,1]$` Estimate`,
                        beta1=as_tibble(predictions$mat_coef)[3,1]$` Estimate`,
                        shape=as_tibble(predictions$mat_coef)[4,1]$` Estimate`,
                        fut_vol=predictions$volatility,
                        var95=var))
}

## Printing Results
tb_forecast_var
```


## Portifolio Optimization

As seen so far (also highlighted in the last table), each return series could be described, in general, by a return rate and volatility. Those parameters could, than, be used to compare the relationship between risk (volatility) and return for each asset. The knowledge of these parameters can be used to decide to invest in one asset over another in order to achieve some desired performance (e.g. maximum return / risk ratio or low risk). An even more interesting approach would be to determine the amounts (percentages) of each asset that, combined, would result the optimal value of the desired metric. The combination of these amounts of each asset will constitute what is called an investment portfolio.
In this section, the objective will be to determine the percentages of each asset studied that would result the the optimal value of the following metrics:

  - Sharpe ratio (return / risk ratio);
  - Minimum risk.

In order to determine the best portfolio, the following code will run 50000 simulations (50000 different portfolios) and select those that match the above metrics by ordering by the desired metric first followed by the maximum return in order to handle possible ties for the desired metric. Details about the functions can be found in the beginning of this document.   

```{r plotting_portfolios, out.width="100%"}
## Converting the original stock return dataset from format long to format wide
df_log_returns_wider <- df_log_returns %>% 
  dplyr::select(ref_date, name=ticker, value=ret_closing_prices) %>% 
  tidyr::pivot_wider()

## Creating a timetk object
df_log_returns_xts <- df_log_returns_wider %>% 
  timetk::tk_xts(select=-ref_date, date_var=ref_date)

## Running simulations
df_port_sim <- portfolio_simulation(nruns=50000, assets=tickers, return_xts=df_log_returns_xts)

## Plotting the weights of each portfolio
plot_min_volatility <- df_port_sim %>%
  arrange(portfolio_volatility, desc(portfolio_return)) %>%
  head(1) %>%
  pivot_longer(2:7) %>% 
  mutate(name = forcats::fct_reorder(name, value)) %>% 
  ggplot(aes(name, value)) +
  geom_col() +
  scale_y_continuous(labels = scales::percent) +
  labs(
    x = "Asset",
    y = "Weight",
    title = "Min variance portfolio weights"
  ) +
  theme_classic() +
  theme(axis.text.y = element_text(size=9),
        axis.text.x = element_text(size=6))

plot_max_sharpe_ratio <- df_port_sim %>%
  arrange(desc(portfolio_sharpe_ratio), desc(portfolio_return)) %>%
  head(1) %>%
  pivot_longer(2:7) %>% 
  mutate(name = forcats::fct_reorder(name, value)) %>% 
  ggplot(aes(name, value)) +
  geom_col() +
  scale_y_continuous(labels = scales::percent) +
  labs(
    x = "Asset",
    y = "Weight",
    title = "Max sharpe ratio portfolio weights"
  ) +
  theme_classic() +
  theme(axis.text.y = element_text(size=9),
        axis.text.x = element_text(size=6))

## Comparision
plot <- ggarrange(plot_min_volatility, 
                  plot_max_sharpe_ratio, 
                  ncol=2, 
                  nrow=1, 
                  widths=c(1,1),
                  legend="bottom",
                  common.legend = T)

annotate_figure(plot, top=text_grob("Portfolio optimization considering minimum volatility and maximum sharpe ratio", 
                                    color="#000000", 
                                    size=14, 
                                    x=unit(0.5, "lines"), 
                                    y=unit(0, "lines"), 
                                    just="left", 
                                    hjust=0, 
                                    vjust=0))
```
The left chart above indicates that if the desire was to obtain the **lowest risk** the best option would be to allocate most of the portfolio in **"HGRE11.SA"** and a only a small percentage in **MGLU3.SA**. This result is in line with what was expected since, as seen in the previous section, the asset **HGRE11.SA** has the lowest volatility while the asset **"MGLU3.SA"** has the second highest volatility. It would be only natural that the portfolio that optimizes based on lower risk would give priority to the less volatile asset to the detriment of assets with higher volatility. However, it is interesting to see that the **PETR4.SA**, which has the highest volatility it is not the stock with the lowest percentage. That could be due to the following:

  - The simulation did not find the best portfolio that would optimize for the lowest risk condition;
  - The portfolio selected would have the same risk as a portfolio with lower percentage of **PETR4.SA**. However, the portfolio selected has a higher return rate;
  - Finally, even though **PETR4.SA** has the highest volatility, the combination of a certain amount of this asset with other assets may lower the volatility to a level that is lower than the minimum considering each asset individually;

The right chart, in turn, suggests that if the desired metric is now the optimum relationship between return and risk (sharpe ratio), the best option would be to allocate the majority of the portfolio in **ENEV3.SA**, followed by **VALE3.SA**, which have high return in average. The chart also suggest a small portion of **HGRE11.SA** and **MGLU3.SA**.  It is interesting to see that both optimization metrics significantly penalizes the asset **MGLU3.SA**. That also makes sense since in addition to the asset having one of the highest volatility, it also has the lowest return among the assets studied.
In summary, considering the observation made, it seems that the simulations is doing a decent work finding the optimum portfolio in each case. There may be even better result, but the ones that were found so far are already satisfactory.
In order to see how these options chosen compare with the other portfolios simulated, the next chart shows the plot of the return against the risk for each simulation. In the figure, the red dot at the top denotes the portfolio that optimizes the Sharpe ratio while the leftmost red dot denotes the portfolio with the lowest risk. It is important to highlight that the return have been annualized by multiplying the daily return by 252.

```{r plotting_efficiency_frontier, out.width="100%"}
df_port_sim %>% 
  ggplot(aes(x = portfolio_volatility, y = portfolio_return, color = portfolio_sharpe_ratio)) +
  geom_point() +
  theme_classic() +
  scale_y_continuous(labels = scales::percent) +
  scale_x_continuous(labels = scales::percent) +
  labs(
    x = 'Risk',
    y = 'Returns',
    title = "Portfolio Optimization & Efficient Frontier"
  ) +
  geom_point(
    aes(x = portfolio_volatility, y = portfolio_return), 
    data = df_port_sim %>% arrange(portfolio_volatility, desc(portfolio_return)) %>% head(1), 
    color = 'red'
  ) +
  geom_point(
    aes(x = portfolio_volatility, y = portfolio_return), 
    data = df_port_sim %>% arrange(desc(portfolio_sharpe_ratio), desc(portfolio_return)) %>% head(1), 
    color = 'red'
  )
```

## VaR do portfolio

Considering the calculation of future volatility, a concept widely used to measure market risk one (a few) steps ahead is the concept of value at risk (VaR). The VaR calculation considers future volatility, a desired confidence level and an invested amount in order to determine the maximum possible value that can be lost through an investment, be it an asset or portfolio, according to the following equation:

\begin{align*}
VaR = r(1) + \frac{t_{\nu}(p)*\sqrt{h_{t}(1)}}{\sqrt{\frac{\nu}{\nu-2}}}
\end{align*}

Where $r(1)$ is one-step forward forecasting of returns, $h_{t}(1)$ is the volatility forecast, $\nu$ is the degree of freedom estimated and $t_{\nu}(p)$ is the p-quantile of Standard Student t-distribution with degrees of freedom.

The following code calculates the VaR for the portfolio with the optimum Sharpe ratio. It can be seen that the value is ... 

```{r var_portfolio_low_risk}
## Fetching the best set of weights considering the best sharpe ratio
best_weights <- df_port_sim %>% 
  arrange(portfolio_volatility, desc(portfolio_return)) %>% 
  head(1) %>%
  pivot_longer(2:7) %>%
  dplyr::select(name, value) %>%
  dplyr::rename(ticker=name, weights=value) %>%
  base::merge(., tb_forecast_var %>% select(ticker, fut_vol, shape), by.x="ticker", by.y="ticker", all.x=F, all.y=F) %>%
  tibble::as_tibble() %>%
  dplyr::mutate(weighted_volatility = weights*fut_vol,
                weighted_shape =  weights*shape)


## Calculating the covariance matrix
cov_mat <- cov(df_log_returns_xts)

## Calculating 
volatility_final <- as.numeric(sqrt(best_weights$weights %*% cov_mat %*% best_weights$weights))

## Calculating the degrees of freedom
nu <- max(best_weights$shape)

## Calculating the t-value from a t-student distribution
tvalue <- qt(.95, nu)


## Calculating the Value at risk (VaR) considering an investiment of R$ 1 million
VaRv <- (sum(best_weights$weighted_volatility) + tvalue * volatility_final / sqrt(nu/(nu-2)))
VaR <- VaRv*10^6

print(paste("The value at risk for the optimum Sharpe Ratio portfolio with 95% confidence (VaR95) is: R$ ", 
            round(VaR,0), 
            " reais for an investment of R$ 1 million (", 
            round(100*VaRv,1),
            "%)", sep=""))
```

```{r var_portfolio_sharpe}
## Fetching the best set of weights considering the best sharpe ratio
best_weights <- df_port_sim %>% 
  arrange(desc(portfolio_sharpe_ratio), desc(portfolio_return)) %>% 
  head(1) %>%
  pivot_longer(2:7) %>%
  dplyr::select(name, value) %>%
  dplyr::rename(ticker=name, weights=value) %>%
  base::merge(., tb_forecast_var %>% select(ticker, fut_vol, shape), by.x="ticker", by.y="ticker", all.x=F, all.y=F) %>%
  tibble::as_tibble() %>%
  dplyr::mutate(weighted_volatility = weights*fut_vol,
                weighted_shape =  weights*shape)


## Calculating the covariance matrix
cov_mat <- cov(df_log_returns_xts)

## Calculating 
volatility_final <- as.numeric(sqrt(best_weights$weights %*% cov_mat %*% best_weights$weights))

## Calculating the degrees of freedom
nu <- max(best_weights$shape)

## Calculating the t-value from a t-student distribution
tvalue <- qt(.95, nu)


## Calculating the Value at risk (VaR) considering an investiment of R$ 1 million
VaRv <- (sum(best_weights$weighted_volatility) + tvalue * volatility_final / sqrt(nu/(nu-2)))
VaR <- VaRv*10^6

print(paste("The value at risk for the optimum Sharpe Ratio portfolio with 95% confidence (VaR95) is: R$ ", 
            round(VaR,0), 
            " reais for an investment of R$ 1 million (", 
            round(100*VaRv,1),
            "%)", sep=""))
```

## CAPM

```{r}
## Reading the log-return of BVSP closing price from a csv file
df_bvsp <- read_csv("20221122_bvsp.csv", show_col_types = F) %>%
    dplyr::mutate(ret_closing_prices = as.numeric(ret_closing_prices)) %>%
    dplyr::filter(ref_date >= min(df_log_returns$ref_date) & ref_date <= end_date) %>%
    dplyr::select(ref_date, ibov = ret_closing_prices)


## Initializing result table
all_betas <- tibble(ticker=as.character(), beta=as.numeric())

## Calculating betas
for (asset in tickers){
  
  all_betas <- all_betas %>%
    bind_rows(., tibble(ticker=asset, beta=calculate_beta(asset=asset)))
}

## Merging with best portfolio information
all_betas <- all_betas %>%
  base::merge(., best_weights %>% select(ticker, weights), by.x="ticker", by.y="ticker", all.x=F, all.y=F) %>%
  dplyr::mutate(weighted_beta = beta*weights)

## Printing individual betas  
all_betas
```

```{r}
## Calculating the beta value for the portfolio
sum(all_betas$weighted_beta)
```


The reason for choosing the **IBOVESPA** index as the benchmark is due to the fact that it is the most used index by the financial market to evaluate the performance of equity investments. The other reason

[1] Hyndman, R.J., & Athanasopoulos, G. (2021) Forecasting: principles and practice, 3rd edition, OTexts: Melbourne, Australia. OTexts.com/fpp3. Accessed on 2022-11-15.

[2] D.D., Portfolio Optimization in R, https://www.codingfinance.com/post/2018-05-31-portfolio-opt-in-r/. Accessed on 2022-11-20.